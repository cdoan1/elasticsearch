{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearch: The Definitive Guide - Python\n",
    "\n",
    "Following the examples in the book, here are Python snippets that achieve the same effect.\n",
    "\n",
    "Documentation for the Python libs:\n",
    "\n",
    "Low-level API:\n",
    "\n",
    "https://elasticsearch-py.readthedocs.io/en/master/index.html\n",
    "\n",
    "Expressive DSL API (more \"Pythonic\")\n",
    "\n",
    "http://elasticsearch-dsl.readthedocs.io/en/latest/index.html\n",
    "\n",
    "Github repo for DSL API:\n",
    "\n",
    "https://github.com/elastic/elasticsearch-dsl-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search, Q\n",
    "from pprint import pprint\n",
    "\n",
    "es = Elasticsearch(\n",
    "    'localhost',\n",
    "    # sniff before doing anything\n",
    "    sniff_on_start=True,\n",
    "    # refresh nodes after a node fails to respond\n",
    "    sniff_on_connection_fail=True,\n",
    "    # and also every 60 seconds\n",
    "    sniffer_timeout=60\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping and Analysis\n",
    "\n",
    "> GET /gb/_mapping/tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gb': {'mappings': {'tweet': {'properties': {'date': {'type': 'date'},\n",
      "                                              'name': {'fields': {'keyword': {'ignore_above': 256,\n",
      "                                                                              'type': 'keyword'}},\n",
      "                                                       'type': 'text'},\n",
      "                                              'tweet': {'fields': {'keyword': {'ignore_above': 256,\n",
      "                                                                               'type': 'keyword'}},\n",
      "                                                        'type': 'text'},\n",
      "                                              'user_id': {'type': 'long'}}}}}}\n"
     ]
    }
   ],
   "source": [
    "res = es.indices.get_mapping(index='gb', doc_type='tweet')\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These queries return the same results because the analyzer has tokenized and normalized the string ```2014-09-15``` to ```2014```, ```09```, and ```15```.\n",
    "\n",
    "> GET /_search?q=2014-09-15        # 12 results !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "res = es.search(q='2014-09-15')\n",
    "print(res['hits']['total'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This search is against the _all meta field and so wherever these values are found (in all tweets), a hit is registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total hits:12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = Search(using=es) \\\n",
    "    .query('match', _all='2014-09-15')\n",
    "response = s.execute()\n",
    "print('Total hits:{}\\n'.format(response['hits']['total']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we change the field explicitly to date, then only the one tweet (with that date) is returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total hits:1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = Search(using=es) \\\n",
    "    .query('match', date='2014-09-15')\n",
    "response = s.execute()\n",
    "print('Total hits:{}\\n'.format(response['hits']['total']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Analyzers\n",
    "\n",
    "We can test analyzers using the _analyze API:\n",
    "```\n",
    "GET /_analyze\n",
    "{\n",
    "  \"analyzer\": \"standard\",\n",
    "  \"text\": \"Text to analyze\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': [{'end_offset': 4,\n",
      "             'position': 0,\n",
      "             'start_offset': 0,\n",
      "             'token': 'text',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 7,\n",
      "             'position': 1,\n",
      "             'start_offset': 5,\n",
      "             'token': 'to',\n",
      "             'type': '<ALPHANUM>'},\n",
      "            {'end_offset': 15,\n",
      "             'position': 2,\n",
      "             'start_offset': 8,\n",
      "             'token': 'analyze',\n",
      "             'type': '<ALPHANUM>'}]}\n"
     ]
    }
   ],
   "source": [
    "text = \"Text to analyze\"\n",
    "res = es.indices.analyze(analyzer='standard', body=text)\n",
    "pprint(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the token is the actual term that will be stored in the inverted index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "to\n",
      "analyze\n"
     ]
    }
   ],
   "source": [
    "for token in res['tokens']:\n",
    "    print(token['token'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Built-in Analyzers (Examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"I want to buy an i-pad and use it to purchase some socks on e-bay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i,want,to,buy,an,i,pad,and,use,it,to,purchase,some,socks,on,e,bay\n"
     ]
    }
   ],
   "source": [
    "#standard\n",
    "analyzed_text = [x['token'] for x in es.indices.analyze(analyzer='standard', body=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i,want,to,buy,an,i,pad,and,use,it,to,purchase,some,socks,on,e,bay\n"
     ]
    }
   ],
   "source": [
    "#simple\n",
    "analyzed_text = [x['token'] for x in es.indices.analyze(analyzer='simple', body=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I,want,to,buy,an,i-pad,and,use,it,to,purchase,some,socks,on,e-bay\n"
     ]
    }
   ],
   "source": [
    "#whitespace\n",
    "analyzed_text = [x['token'] for x in es.indices.analyze(analyzer='whitespace', body=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i,want,bui,i,pad,us,purchas,some,sock,e,bai\n"
     ]
    }
   ],
   "source": [
    "#english (language)\n",
    "analyzed_text = [x['token'] for x in es.indices.analyze(analyzer='english', body=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
