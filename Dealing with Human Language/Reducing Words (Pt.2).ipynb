{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearch: The Definitive Guide - Python\n",
    "\n",
    "Following the examples in the book, here are Python snippets that achieve the same effect.\n",
    "\n",
    "Documentation for the Python libs:\n",
    "\n",
    "Low-level API:\n",
    "\n",
    "https://elasticsearch-py.readthedocs.io/en/master/index.html\n",
    "\n",
    "Expressive DSL API (more \"Pythonic\")\n",
    "\n",
    "http://elasticsearch-dsl.readthedocs.io/en/latest/index.html\n",
    "\n",
    "Github repo for DSL API:\n",
    "\n",
    "https://github.com/elastic/elasticsearch-dsl-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 items created\n"
     ]
    }
   ],
   "source": [
    "import index\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search, Q\n",
    "from pprint import pprint\n",
    "\n",
    "es = Elasticsearch(\n",
    "    'localhost',\n",
    "    # sniff before doing anything\n",
    "    sniff_on_start=True,\n",
    "    # refresh nodes after a node fails to respond\n",
    "    sniff_on_connection_fail=True,\n",
    "    # and also every 60 seconds\n",
    "    sniffer_timeout=60\n",
    ")\n",
    "\n",
    "r = index.populate()\n",
    "print('{} items created'.format(len(r['items'])))\n",
    "\n",
    "# Let's repopulate the index as we deleted 'gb' in earlier chapters:\n",
    "# Run the script: populate.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reducing Words to Their Root Form\n",
    "\n",
    "#### Dictionary Stemmers\n",
    "\n",
    "Dictionary stemmers work quite differently from algorithmic stemmers. Instead of applying a standard set of rules to each word, they simply look up the word in the dictionary. Theoretically, they could produce much better results than an algorithmic stemmer. A dictionary stemmer should be able to do the following:\n",
    "\n",
    "* Return the correct root word for irregular forms such as feet and mice\n",
    "* Recognize the distinction between words that are similar but have different word senses—for example, organ and organization\n",
    "\n",
    "**Dictionary Stemmer** - only as good as its dictionary. Most e-dictionaries only ~10% of full dictionaries. Have to be updated etc.\n",
    "\n",
    "**Size and performance** - A dictionary stemmer needs to load all words, all prefixes, and all suffixes into memory. This can use a significant amount of RAM. Finding the right stem for a word is often considerably more complex than the equivalent process with an algorithmic stemmer.\n",
    "\n",
    "Let's explore the Hunspell dictionary \"stemmer\":\n",
    "\n",
    "```\n",
    "config/\n",
    "  └ hunspell/ \n",
    "      └ en_GB/ \n",
    "          ├ en_GB.dic\n",
    "          ├ en_GB.aff\n",
    "          └ settings.yml \n",
    "```\n",
    "\n",
    "Note that we don't need to touch settings.yml (which override any settings in the master settings file: ```elasticsearch.yml```. Settings can be used to ignore case, which is otherwise set to false. \n",
    "\n",
    "* ```indices.analysis.hunspell.dictionary.ignore_case```\n",
    "\n",
    "(NOTE: due to my British roots, I changed the example to use the GB dictionary noting that the US version is derived from it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"analysis\" : {\n",
    "        \"analyzer\" : {\n",
    "            \"en_GB\" : {\n",
    "                \"tokenizer\" : \"standard\",\n",
    "                \"filter\" : [ \"lowercase\", \"en_GB\" ]\n",
    "            }\n",
    "        },\n",
    "        \"filter\" : {\n",
    "            \"en_GB\" : {\n",
    "                \"type\" : \"hunspell\",\n",
    "                \"locale\" : \"en_GB\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "index.create_my_index(body=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you'r,right,about,organ,jack,über,gener,waiter\n"
     ]
    }
   ],
   "source": [
    "# test with the standard English analyzer\n",
    "text = \"You're right about organizing jack's Über generation of waiters.\" \n",
    "analyzed_text = [x['token'] for x in es.indices.analyze\\\n",
    "                 (index='my_index', analyzer='english', text=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you're,right,about,organize,organ,jack,über,generation,generate,genera,of,wait\n"
     ]
    }
   ],
   "source": [
    "analyzed_text = [x['token'] for x in es.indices.analyze\\\n",
    "                 (index='my_index', analyzer='en_GB', text=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens with the following words that are known to be overstemmed by Porter stemmers (and later improved by the Porter2 stemmer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gener,gener,gener,gener,organ,waiter\n"
     ]
    }
   ],
   "source": [
    "text = \"A generically generally generously generated organized waiter.\"\n",
    "# English\n",
    "analyzed_text = [x['token'] for x in es.indices.analyze\\\n",
    "                 (index='my_index', analyzer='english', text=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a,genera,genera,generously,generous,generate,organize,organ,wait\n"
     ]
    }
   ],
   "source": [
    "# en_GB Hunspell:\n",
    "analyzed_text = [x['token'] for x in es.indices.analyze\\\n",
    "                 (index='my_index', analyzer='en_GB', text=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_token_filter = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"filter\": {\n",
    "        \"english_stop\": {\n",
    "          \"type\":       \"stop\",\n",
    "          \"stopwords\":  \"_english_\"\n",
    "        },\n",
    "        \"light_english_stemmer\": {\n",
    "          \"type\":       \"stemmer\",\n",
    "          \"language\":   \"light_english\" \n",
    "        },\n",
    "        \"english_possessive_stemmer\": {\n",
    "          \"type\":       \"stemmer\",\n",
    "          \"language\":   \"possessive_english\"\n",
    "        }\n",
    "      },\n",
    "      \"analyzer\": {\n",
    "        \"my_english\": {\n",
    "          \"tokenizer\":  \"standard\",\n",
    "          \"filter\": [\n",
    "            \"english_possessive_stemmer\",\n",
    "            \"lowercase\",\n",
    "            \"english_stop\",\n",
    "            \"light_english_stemmer\", \n",
    "            \"asciifolding\" \n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "index.create_my_index(body=english_token_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generic,generally,generous,generate,organized,waiter\n"
     ]
    }
   ],
   "source": [
    "# my_english custom analyzer:\n",
    "analyzed_text = [x['token'] for x in es.indices.analyze\\\n",
    "                 (index='my_index', analyzer='my_english', text=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "porter_token_filter = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"filter\": {\n",
    "        \"english_stop\": {\n",
    "          \"type\":       \"stop\",\n",
    "          \"stopwords\":  \"_english_\"\n",
    "        },\n",
    "        \"porter\": {\n",
    "          \"type\":       \"stemmer\",\n",
    "          \"language\":   \"porter\" \n",
    "        },\n",
    "        \"english_possessive_stemmer\": {\n",
    "          \"type\":       \"stemmer\",\n",
    "          \"language\":   \"possessive_english\"\n",
    "        }\n",
    "      },\n",
    "      \"analyzer\": {\n",
    "        \"my_porter_english\": {\n",
    "          \"tokenizer\":  \"standard\",\n",
    "          \"filter\": [\n",
    "            \"english_possessive_stemmer\",\n",
    "            \"lowercase\",\n",
    "            \"english_stop\",\n",
    "            \"porter\", \n",
    "            \"asciifolding\" \n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "index.create_my_index(body=porter_token_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gener,gener,gener,gener,organ,waiter\n"
     ]
    }
   ],
   "source": [
    "# my_english custom analyzer:\n",
    "analyzed_text = [x['token'] for x in es.indices.analyze\\\n",
    "                 (index='my_index', analyzer='my_porter_english', text=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "porter2_token_filter = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"filter\": {\n",
    "        \"english_stop\": {\n",
    "          \"type\":       \"stop\",\n",
    "          \"stopwords\":  \"_english_\"\n",
    "        },\n",
    "        \"porter2\": {\n",
    "          \"type\":       \"stemmer\",\n",
    "          \"language\":   \"porter2\" \n",
    "        },\n",
    "        \"english_possessive_stemmer\": {\n",
    "          \"type\":       \"stemmer\",\n",
    "          \"language\":   \"possessive_english\"\n",
    "        }\n",
    "      },\n",
    "      \"analyzer\": {\n",
    "        \"my_porter2_english\": {\n",
    "          \"tokenizer\":  \"standard\",\n",
    "          \"filter\": [\n",
    "            \"english_possessive_stemmer\",\n",
    "            \"lowercase\",\n",
    "            \"english_stop\",\n",
    "            \"porter2\", \n",
    "            \"asciifolding\" \n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "index.create_my_index(body=porter2_token_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generic,general,generous,generat,organ,waiter\n"
     ]
    }
   ],
   "source": [
    "# my_english custom analyzer:\n",
    "analyzed_text = [x['token'] for x in es.indices.analyze\\\n",
    "                 (index='my_index', analyzer='my_porter2_english', text=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Analyzer Comparison:\n",
    "\n",
    "text = \"A generically generally generously generated organized waiter.\"\n",
    "\n",
    "##### English\n",
    "\n",
    "gener,gener,gener,gener,organ,waiter\n",
    "\n",
    "##### Hunspell (en_GB) #####\n",
    "\n",
    "a,genera,genera,generously,generous,generate,organize,organ,wait\n",
    "\n",
    "##### \"My English\" (Lite stemmer)\n",
    "\n",
    "generic,generally,generous,generate,organized,waiter\n",
    "\n",
    "##### \"My English\" (Porter stemmer)\n",
    "\n",
    "gener,gener,gener,gener,organ,waiter\n",
    "\n",
    "##### \"My English\" (Porter2 stemmer)\n",
    "\n",
    "generic,general,generous,generat,organ,waiter\n",
    "\n",
    "\n",
    "### Preventing Stemming\n",
    "\n",
    "Maybe important to keep skies and skiing as distinct words rather than stemming them both down to ski (as would happen with the english analyzer).\n",
    "\n",
    "The ```keyword_marker``` and ```stemmer_override``` token filters customize the stemming process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stem_control_settings = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"filter\": {\n",
    "        \"no_stem\": {\n",
    "          \"type\": \"keyword_marker\",\n",
    "          \"keywords\": [ \"skies\" ] \n",
    "        }\n",
    "      },\n",
    "      \"analyzer\": {\n",
    "        \"my_stemmer\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "            \"no_stem\",\n",
    "            \"porter_stem\"\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "index.create_my_index(body=stem_control_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sky,skies,ski,ski\n"
     ]
    }
   ],
   "source": [
    "# my_stemmer custom analyzer:\n",
    "text = ['sky skies skiing skis']\n",
    "analyzed_text = [x['token'] for x in es.indices.analyze\\\n",
    "                 (index='my_index', analyzer='my_stemmer', text=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the language analyzers allow us only to specify an array of words in the stem_exclusion parameter, the keyword_marker token filter also accepts a keywords_path parameter that allows us to store all of our keywords in [a file](https://www.elastic.co/guide/en/elasticsearch/guide/master/using-stopwords.html#updating-stopwords).\n",
    "\n",
    "#### Customizing Stemming\n",
    "\n",
    "Perhaps we prefer \"skies\" to be stemmed to \"sky\" instead. The ```stemmer_override``` token filter allows us to specify our own custom stemming rules. At the same time, we can handle some irregular forms like stemming mice to mouse and feet to foot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the,mouse,came,down,from,the,sky,and,ran,over,my,foot\n"
     ]
    }
   ],
   "source": [
    "my_stemmer_override = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"filter\": {\n",
    "        \"custom_stem\": {\n",
    "          \"type\": \"stemmer_override\",\n",
    "          \"rules\": [ \n",
    "            \"skies=>sky\",\n",
    "            \"mice=>mouse\",\n",
    "            \"feet=>foot\"\n",
    "          ]\n",
    "        }\n",
    "      },\n",
    "      \"analyzer\": {\n",
    "        \"my_stemmer_override\": {\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "            \"custom_stem\", \n",
    "            \"porter_stem\"\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "index.create_my_index(body=my_stemmer_override)\n",
    "# my_stemmer_override custom analyzer:\n",
    "text = ['The mice came down from the skies and ran over my feet']\n",
    "analyzed_text = [x['token'] for x in es.indices.analyze\\\n",
    "                 (index='my_index', analyzer='my_stemmer_override', text=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: The stemmer_override filter (\"custom_stem\") must be placed **before** the stemmer (here \"porter_stem\").\n",
    "\n",
    "Just as for the keyword_marker token filter, rules can be stored in a file whose location should be specified with the ```rules_path``` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
