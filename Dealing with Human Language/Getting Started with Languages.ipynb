{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearch: The Definitive Guide - Python\n",
    "\n",
    "Following the examples in the book, here are Python snippets that achieve the same effect.\n",
    "\n",
    "Documentation for the Python libs:\n",
    "\n",
    "Low-level API:\n",
    "\n",
    "https://elasticsearch-py.readthedocs.io/en/master/index.html\n",
    "\n",
    "Expressive DSL API (more \"Pythonic\")\n",
    "\n",
    "http://elasticsearch-dsl.readthedocs.io/en/latest/index.html\n",
    "\n",
    "Github repo for DSL API:\n",
    "\n",
    "https://github.com/elastic/elasticsearch-dsl-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 items created\n"
     ]
    }
   ],
   "source": [
    "import index\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search, Q\n",
    "from pprint import pprint\n",
    "\n",
    "es = Elasticsearch(\n",
    "    'localhost',\n",
    "    # sniff before doing anything\n",
    "    sniff_on_start=True,\n",
    "    # refresh nodes after a node fails to respond\n",
    "    sniff_on_connection_fail=True,\n",
    "    # and also every 60 seconds\n",
    "    sniffer_timeout=60\n",
    ")\n",
    "\n",
    "r = index.populate()\n",
    "print('{} items created'.format(len(r['items'])))\n",
    "\n",
    "# Let's repopulate the index as we deleted 'gb' in earlier chapters:\n",
    "# Run the script: populate.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started with Languages\n",
    "\n",
    "Full-text search is a battle between precision—returning as few irrelevant documents as possible—and recall—returning as many relevant documents as possible.\n",
    "\n",
    "Many tactics can be deployed to tackle precision and recall, such as modifying words: e.g. search for \"jumping\", \"jumps\" and \"jumped\" by reducing words to their stem (root form) - \"jump\".\n",
    "\n",
    "However, the first step is to identify words using an analyzer:\n",
    "\n",
    "##### Tokenize text into individual words:\n",
    "\n",
    "```The quick brown foxes → [The, quick, brown, foxes]```\n",
    "\n",
    "##### Lowercase tokens:\n",
    "\n",
    "```The → the```\n",
    "\n",
    "##### Remove common stopwords:\n",
    "\n",
    "```[The, quick, brown, foxes] → [quick, brown, foxes]```\n",
    "\n",
    "##### Stem tokens to their root form:\n",
    "\n",
    "```foxes → fox```\n",
    "\n",
    "Each analyzer may also apply other transformations specific to its language in order to make words from that language more searchable:\n",
    "\n",
    "##### The english analyzer removes the possessive 's:\n",
    "\n",
    "```John's → john```\n",
    "\n",
    "##### The french analyzer removes elisions like l' and qu' and diacritics like ¨ or ^:\n",
    "\n",
    "```l'église → eglis```\n",
    "\n",
    "##### The german analyzer normalizes terms, replacing ä and ae with a, or ß with ss, among others:\n",
    "\n",
    "```äußerst → ausserst```\n",
    "\n",
    "### Using Language Analyzers\n",
    "\n",
    "The built-in language analyzers are available globally and don’t need to be configured before being used. They can be specified directly in the field mapping:\n",
    "\n",
    "```\n",
    "PUT /my_index\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"blog\": {\n",
    "      \"properties\": {\n",
    "        \"title\": {\n",
    "          \"type\":     \"string\",\n",
    "          \"analyzer\": \"english\" \n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm,happi,about,fox\n"
     ]
    }
   ],
   "source": [
    "#english (language)\n",
    "text = 'I\\'m not happy about the foxes'\n",
    "analyzed_text = [x['token'] for x in es.indices.analyze\\\n",
    "                 (analyzer='english', body=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can’t tell if a document mentions one fox or many foxes; the word 'not' is a stopword and is removed, so we can’t tell whether the document is happy about foxes or not. By using the english analyzer, we have **increased recall** as we can match more loosely, but we have reduced our ability to rank documents accurately.\n",
    "\n",
    "To get the best of both worlds, we can use multifields to index the title field twice: once with the english analyzer and once with the standard analyzer:\n",
    "\n",
    "```\n",
    "PUT /my_index\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"blog\": {\n",
    "      \"properties\": {\n",
    "        \"title\": { \n",
    "          \"type\": \"string\",\n",
    "          \"fields\": {\n",
    "            \"english\": { \n",
    "              \"type\":     \"string\",\n",
    "              \"analyzer\": \"english\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_template = {\n",
    "  \"mappings\": {\n",
    "    \"blog\": {\n",
    "      \"properties\": {\n",
    "        \"title\": { \n",
    "          \"type\": \"text\",\n",
    "          \"fields\": {\n",
    "            \"english\": { \n",
    "              \"type\":     \"text\",\n",
    "              \"analyzer\": \"english\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.create(index='my_index', body=index_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '1',\n",
       " '_index': 'my_index',\n",
       " '_shards': {'failed': 0, 'successful': 1, 'total': 2},\n",
       " '_type': 'blog',\n",
       " '_version': 1,\n",
       " 'created': True,\n",
       " 'result': 'created'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = { \"title\": \"I'm happy for this fox\" }\n",
    "es.create(index='my_index', doc_type='blog', body=data, id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '2',\n",
       " '_index': 'my_index',\n",
       " '_shards': {'failed': 0, 'successful': 1, 'total': 2},\n",
       " '_type': 'blog',\n",
       " '_version': 1,\n",
       " 'created': True,\n",
       " 'result': 'created'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = { \"title\": \"I'm not happy about my fox problem\" }\n",
    "es.create(index='my_index', doc_type='blog', body=data, id=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not happy about my fox problem\n",
      "I'm happy for this fox\n"
     ]
    }
   ],
   "source": [
    "s = Search(using=es, index='my_index', doc_type='blog')\n",
    "q = Q('multi_match', type='most_fields', query='not happy foxes', fields=['title', 'title.english'])\n",
    "s = s.query()\n",
    "res = s.execute()\n",
    "for hit in res:\n",
    "    print(hit.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both hits **do not** contain the word foxes, but we got a hit on fox.\n",
    "\n",
    "Use the ```most_fields``` query type to match the same text in as many fields as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Lanuage Analyzers\n",
    "\n",
    "It might be useful to avoid stemming words (like \"organization\" --> organ) if you know this will sacrifice certain precision requirements (e.g. seaches for \"world health organization\"). It is possible to configure the analyzers, e.g. to exclude certain stop words or stems:\n",
    "\n",
    "```\n",
    "PUT /my_index\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"my_english\": {\n",
    "          \"type\": \"english\",\n",
    "          \"stem_exclusion\": [ \"organization\", \"organizations\" ], \n",
    "          \"stopwords\": [ \n",
    "            \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\",\n",
    "            \"if\", \"in\", \"into\", \"is\", \"it\", \"of\", \"on\", \"or\", \"such\", \"that\",\n",
    "            \"the\", \"their\", \"then\", \"there\", \"these\", \"they\", \"this\", \"to\",\n",
    "            \"was\", \"will\", \"with\"\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "GET /my_index/_analyze?analyzer=my_english \n",
    "The World Health Organization does not sell organs.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.delete(index='my_index')\n",
    "index_template_with_exclusions = \\\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"my_english\": {\n",
    "          \"type\": \"english\",\n",
    "          \"stem_exclusion\": [ \"organization\", \"organizations\" ], \n",
    "          \"stopwords\": [ \n",
    "            \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\",\n",
    "            \"if\", \"in\", \"into\", \"is\", \"it\", \"of\", \"on\", \"or\", \"such\", \"that\",\n",
    "            \"the\", \"their\", \"then\", \"there\", \"these\", \"they\", \"this\", \"to\",\n",
    "            \"was\", \"will\", \"with\"\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "es.indices.create(index='my_index', body=index_template_with_exclusions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world,health,organization,doe,not,sell,organ\n"
     ]
    }
   ],
   "source": [
    "#english (language) with exclusions - my_english\n",
    "text = 'The World Health Organization does not sell organs.'\n",
    "analyzed_text = [x['token'] for x in es.indices.analyze\\\n",
    "                 (index='my_index', analyzer='my_english', body=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
