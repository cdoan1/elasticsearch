{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearch: The Definitive Guide - Python\n",
    "\n",
    "Following the examples in the book, here are Python snippets that achieve the same effect.\n",
    "\n",
    "Documentation for the Python libs:\n",
    "\n",
    "Low-level API:\n",
    "\n",
    "https://elasticsearch-py.readthedocs.io/en/master/index.html\n",
    "\n",
    "Expressive DSL API (more \"Pythonic\")\n",
    "\n",
    "http://elasticsearch-dsl.readthedocs.io/en/latest/index.html\n",
    "\n",
    "Github repo for DSL API:\n",
    "\n",
    "https://github.com/elastic/elasticsearch-dsl-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 items created\n"
     ]
    }
   ],
   "source": [
    "import index\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search, Q\n",
    "from pprint import pprint\n",
    "\n",
    "es = Elasticsearch(\n",
    "    'localhost',\n",
    "    # sniff before doing anything\n",
    "    sniff_on_start=True,\n",
    "    # refresh nodes after a node fails to respond\n",
    "    sniff_on_connection_fail=True,\n",
    "    # and also every 60 seconds\n",
    "    sniffer_timeout=60\n",
    ")\n",
    "\n",
    "r = index.populate()\n",
    "print('{} items created'.format(len(r['items'])))\n",
    "\n",
    "# Let's repopulate the index as we deleted 'gb' in earlier chapters:\n",
    "# Run the script: populate.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Stop Words\n",
    "\n",
    "#### Stopwords and the Standard Analyzer\n",
    "\n",
    "To use custom stopwords in conjunction with the standard analyzer, all we need to do is to create a configured version of the analyzer and pass in the list of stopwords that we require:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "settings = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"my_analyzer\": { \n",
    "          \"type\": \"standard\", \n",
    "          \"stopwords\": [ \"and\", \"the\" ] \n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "index.create_my_index(body=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the,quick,and,the,dead\n"
     ]
    }
   ],
   "source": [
    "# test with the __standard__analyzer\n",
    "text = \"The quick and the dead.\" \n",
    "analyzed_text = [x['token'] for x in es.indices.analyze\\\n",
    "                 (index='my_index', analyzer='standard', text=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick,dead\n"
     ]
    }
   ],
   "source": [
    "# test with my_analyzer\n",
    "text = \"The quick and the dead.\" \n",
    "analyzed_text = [x['token'] for x in es.indices.analyze\\\n",
    "                 (index='my_index', analyzer='my_analyzer', text=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [{'end_offset': 9,\n",
       "   'position': 1,\n",
       "   'start_offset': 4,\n",
       "   'token': 'quick',\n",
       "   'type': '<ALPHANUM>'},\n",
       "  {'end_offset': 22,\n",
       "   'position': 4,\n",
       "   'start_offset': 18,\n",
       "   'token': 'dead',\n",
       "   'type': '<ALPHANUM>'}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that the word positions (quick - pos 1, dead - pos 4) have been maintained:\n",
    "es.indices.analyze(index='my_index', analyzer='my_analyzer', text=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word position integrity (per above example) is important for phrase queries â€” if the positions of each term had been adjusted, a phrase query for quick dead would have matched the preceding example incorrectly.\n",
    "\n",
    "Note that the ```stopwords``` field accepts a range of settings:\n",
    "\n",
    "##### Array of stop words\n",
    "\n",
    "> ```\"stopwords\": [ \"and\", \"the\" ]```\n",
    "\n",
    "##### Default language stopwords\n",
    "\n",
    "> ```\"stopwords\": \"_english_\"```\n",
    "\n",
    "##### No stopwords\n",
    "\n",
    "> ```\"stopwords\": \"_none_\"```\n",
    "\n",
    "The default stopwords for ```_english_```:\n",
    "\n",
    "```a, an, and, are, as, at, be, but, by, for, if, in, into, is, it,\n",
    "no, not, of, on, or, such, that, the, their, then, there, these,\n",
    "they, this, to, was, will, with```\n",
    "\n",
    "Note that stopwords can be placed in a file (default config/stopwords).\n",
    "I placed a file <es-home>/config/stopwords/english.txt with contents:\n",
    "```\n",
    "a\n",
    "the\n",
    "dead\n",
    "```\n",
    "i.e. one stopword per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "settings={\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"my_english\": {\n",
    "          \"type\":           \"english\",\n",
    "          \"stopwords_path\": \"stopwords/english.txt\" \n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "index.create_my_index(body=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick,and,is,good,film\n"
     ]
    }
   ],
   "source": [
    "# test with my_analyzer\n",
    "text = \"The quick and the dead is a good film.\" \n",
    "analyzed_text = [x['token'] for x in es.indices.analyze\\\n",
    "                 (index='my_index', analyzer='my_english', text=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
