{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearch: The Definitive Guide - Python\n",
    "\n",
    "Following the examples in the book, here are Python snippets that achieve the same effect.\n",
    "\n",
    "Documentation for the Python libs:\n",
    "\n",
    "Low-level API:\n",
    "\n",
    "https://elasticsearch-py.readthedocs.io/en/master/index.html\n",
    "\n",
    "Expressive DSL API (more \"Pythonic\")\n",
    "\n",
    "http://elasticsearch-dsl.readthedocs.io/en/latest/index.html\n",
    "\n",
    "Github repo for DSL API:\n",
    "\n",
    "https://github.com/elastic/elasticsearch-dsl-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 items created\n"
     ]
    }
   ],
   "source": [
    "import index\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search, Q\n",
    "from pprint import pprint\n",
    "\n",
    "es = Elasticsearch(\n",
    "    'localhost',\n",
    "    # sniff before doing anything\n",
    "    sniff_on_start=True,\n",
    "    # refresh nodes after a node fails to respond\n",
    "    sniff_on_connection_fail=True,\n",
    "    # and also every 60 seconds\n",
    "    sniffer_timeout=60\n",
    ")\n",
    "\n",
    "r = index.populate()\n",
    "print('{} items created'.format(len(r['items'])))\n",
    "\n",
    "# Let's repopulate the index as we deleted 'gb' in earlier chapters:\n",
    "# Run the script: populate.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Identifying Words\n",
    "\n",
    "A word in English is relatively simple to spot: words are separated by whitespace or (some) punctuation. Even in English, though, there can be controversy: is you’re one word or two? What about o’clock, cooperate, half-baked, or eyewitness?\n",
    "\n",
    "The standard analyzer is used by default for any full-text analyzed string field. If we were to reimplement the standard analyzer as a custom analyzer, it would be defined as follows:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"type\":      \"custom\",\n",
    "    \"tokenizer\": \"standard\",\n",
    "    \"filter\":  [ \"lowercase\", \"stop\" ]\n",
    "}\n",
    "```\n",
    "\n",
    "#### Standard Tokenizer\n",
    "\n",
    "What is interesting is the algorithm that is used to identify words. The whitespace tokenizer simply breaks on whitespace—spaces, tabs, line feeds, and so forth—and assumes that contiguous nonwhitespace characters form a single token. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're,the,1st,runner,home!\n"
     ]
    }
   ],
   "source": [
    "# Whitespace tokenizer\n",
    "text = \"You're the 1st runner home!\"\n",
    "analyzed_text = [x['token'] for x in es.indices.analyze\\\n",
    "                 (tokenizer='whitespace', body=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're,my,co,opted,favorite,cool_dude\n"
     ]
    }
   ],
   "source": [
    "# Standard tokenizer - uses Unicode Text Segmentation standard\n",
    "text = \"You're my co-opted 'favorite' cool_dude.\" # single quotes 'favorite'\n",
    "analyzed_text = [x['token'] for x in es.indices.analyze\\\n",
    "                 (tokenizer='standard', body=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're,my,co,opted,favorite,cool_dude,Pls,email,me,friend,dude.it\n"
     ]
    }
   ],
   "source": [
    "# Standard tokenizer - uses Unicode Text Segmentation standard\n",
    "# Note that string contains an email address\n",
    "text = \"You're my co-opted 'favorite' cool_dude. Pls email me friend@dude.it\"\n",
    "analyzed_text = [x['token'] for x in es.indices.analyze\\\n",
    "                 (tokenizer='standard', body=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're,my,co,opted,favorite,cool_dude,Pls,email,me,friend@dude.it\n"
     ]
    }
   ],
   "source": [
    "# Standard tokenizer - uses Unicode Text Segmentation standard\n",
    "text = \"You're my co-opted 'favorite' cool_dude. Pls email me friend@dude.it\"\n",
    "analyzed_text = [x['token'] for x in es.indices.analyze\\\n",
    "                 (tokenizer='uax_url_email', text=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard tokenizer is a reasonable starting point for tokenizing most languages, especially Western languages. In fact, it forms the basis of most of the language-specific analyzers like the english, french, and spanish analyzers. Its support for Asian languages, however, is limited, and you should consider using the icu_tokenizer instead, which is available in the ICU plug-in.\n",
    "\n",
    "#### Tidying Up Input Text\n",
    "\n",
    "Tokenizers produce the best results when the input text is clean, valid text, where valid means that it follows the punctuation rules that the Unicode algorithm expects. Quite often, though, the text we need to process is anything but clean. Cleaning it up before tokenization improves the quality of the output.\n",
    "\n",
    "For example, HTML can get messy...\n",
    "\n",
    "```\n",
    "GET /_analyze?tokenizer=standard\n",
    "<p>Some d&eacute;j&agrave; vu <a href=\"http://somedomain.com>\">website</a>\n",
    "```\n",
    "\n",
    "To use them as part of the analyzer, they should be added to a custom analyzer definition:\n",
    "\n",
    "```\n",
    "PUT /my_index\n",
    "{\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"my_html_analyzer\": {\n",
    "                    \"tokenizer\":     \"standard\",\n",
    "                    \"char_filter\": [ \"html_strip\" ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = '<p>Some d&eacute;j&agrave; vu <a href=\"http://somedomain.com>\">website</a>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from elasticsearch_dsl import analyzer, Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_custom_analyzer = analyzer('my_html_analyzer',\n",
    "        tokenizer='standard',\n",
    "        char_filter='html_strip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = Index('my_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i.analyzer(my_custom_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some,déjà,vu,website\n"
     ]
    }
   ],
   "source": [
    "analyzed_text = [x['token'] for x in es.indices.analyze\\\n",
    "                 (index='my_index', analyzer='my_html_analyzer', text=text)['tokens']]\n",
    "print(','.join(analyzed_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: I cheated here because the above method call returned an illegal exception that I was unable to debug (related to passing in the char_filter param). So I created the index using the above params "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
